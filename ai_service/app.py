import os
import logging
import unicodedata  # üîß M·ªöI: h·ªó tr·ª£ chu·∫©n h√≥a ti·∫øng Vi·ªát kh√¥ng d·∫•u
from fastapi import FastAPI, Body, Query
from fastapi.responses import JSONResponse
from sentence_transformers import SentenceTransformer
import psycopg2
from PyPDF2 import PdfReader
import docx 
from typing import Union, List
import numpy as np
import re

# T·∫°o th∆∞ m·ª•c logs n·∫øu ch∆∞a c√≥
if not os.path.exists("logs"):
    os.makedirs("logs")

# Thi·∫øt l·∫≠p ghi log
logging.basicConfig(
    filename="logs/upload.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = FastAPI()
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  # 768 chi·ªÅu 

# K·∫øt n·ªëi PostgreSQL
conn = psycopg2.connect(
    host="semantic_search_db",  
    port=5432,
    user="admin",
    password="123456",
    database="StudentFormDB"
)
cursor = conn.cursor()

# Helper function ƒë·ªÉ ƒë·ªçc file PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PdfReader(file)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text
    return text

# Helper function ƒë·ªÉ ƒë·ªçc file DOCX
def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

# üîß M·ªöI: Chu·∫©n h√≥a ti·∫øng Vi·ªát (b·ªè d·∫•u, lowercase)
def normalize_text(text):
    """
    ‚úÖ Chu·∫©n h√≥a ti·∫øng Vi·ªát: b·ªè d·∫•u, chuy·ªÉn v·ªÅ lowercase
    """
    text = unicodedata.normalize('NFD', text)
    text = ''.join([c for c in text if unicodedata.category(c) != 'Mn'])
    return text.lower()

# üîß M·ªöI: Mapping t·ª´ vi·∫øt t·∫Øt ho·∫∑c ti·∫øng Anh ph·ªï bi·∫øn
SYNONYM_MAP = {
    "hocbong": "h·ªçc b·ªïng",
    "hoclai": "h·ªçc l·∫°i",
    "nghihoc": "ngh·ªâ h·ªçc",
    "report": "t∆∞·ªùng tr√¨nh",
    "dt": "ƒëi·ªán tho·∫°i",
    "sv": "sinh vi√™n",
    "gv": "gi·∫£ng vi√™n",
    "cancel": "h·ªßy",
    "apply": "xin",
    "form": "bi·ªÉu m·∫´u",
}

def normalize_and_expand(text):
    """
    ‚úÖ Chu·∫©n h√≥a + x·ª≠ l√Ω vi·∫øt li·ªÅn + d·ªãch vi·∫øt t·∫Øt/ti·∫øng Anh
    """
    text = normalize_text(text.replace(" ", ""))
    return SYNONYM_MAP.get(text, text)

# H√†m ki·ªÉm tra xem t√™n file ƒë√£ t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu ch∆∞a
def is_file_exists(title):  
    cursor.execute("SELECT COUNT(*) FROM forms WHERE title = %s", (title,))
    count = cursor.fetchone()[0]
    return count > 0

# H√†m x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file
def delete_duplicate_files():
    try:
        cursor.execute("""
            DELETE FROM forms
            WHERE ctid NOT IN (
                SELECT min(ctid)
                FROM documents
                GROUP BY title
            );
        """)
        conn.commit()
        logging.info("ƒê√£ x√≥a t·∫•t c·∫£ c√°c b·∫£n ghi tr√πng t√™n file.")
    except Exception as e:
        logging.error(f"L·ªói khi x√≥a file tr√πng: {e}")
        conn.rollback()

# H√†m upload file (d√πng chung cho API v√† auto load)
def upload_file_to_db(file_path):
    title = os.path.basename(file_path)
    
    if is_file_exists(title):
        logging.info(f"File '{title}' ƒë√£ t·ªìn t·∫°i. X√≥a file tr√πng...")
        delete_duplicate_files()
    
    content = ""
    if file_path.endswith(".pdf"):
        content = extract_text_from_pdf(file_path)
    elif file_path.endswith(".docx"):
        content = extract_text_from_docx(file_path)
    else:
        logging.warning(f"B·ªè qua file kh√¥ng h·ªó tr·ª£: {file_path}")
        return {"error": "Unsupported file type"}
    
    embedding = model.encode(content).tolist()
    
    try:
        cursor.execute(
            "INSERT INTO forms (title, content, embedding) VALUES (%s, %s, %s)",
            (title, content, embedding)
        )
        conn.commit()
        logging.info(f"Uploaded: {title}")
        return {"status": "uploaded", "title": title}
    except Exception as e:
        logging.error(f"L·ªói khi upload file: {e}")
        conn.rollback()
        return {"error": f"Failed to upload file: {e}"}

#API/Search
@app.get("/search")
def semantic_search(
    query: str = Query(..., description="C√¢u truy v·∫•n t√¨m ki·∫øm"),
    top_k: int = Query(5, description="S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t·ªëi ƒëa mu·ªën tr·∫£ v·ªÅ")
):
    """
    ‚úÖ API Semantic Search n√¢ng cao:

    üéØ T√≠nh nƒÉng ch√≠nh (C·∫¨P NH·∫¨T):
    1. ∆Øu ti√™n so kh·ªõp v·ªõi ti√™u ƒë·ªÅ bi·ªÉu m·∫´u ng∆∞·ªùi d√πng nh·ªõ
    2. N·ªôi dung bi·ªÉu m·∫´u ƒë∆∞·ª£c so kh·ªõp k·ªπ h∆°n v·ªõi c√¢u m√¥ t·∫£
    3. T√™n file ch·ªâ ƒë∆∞·ª£c ∆∞u ti√™n n·∫øu ng∆∞·ªùi d√πng nh·∫≠p g·∫ßn gi·ªëng t√™n file
    4. SBERT ƒë∆∞·ª£c d√πng ƒë·ªÉ hi·ªÉu √Ω ƒë·∫°i di·ªán (ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß)
    5. K·∫øt h·ª£p t·∫•t c·∫£ ti√™u ch√≠ ƒë·ªÉ t√≠nh ƒëi·ªÉm v√† l·ªçc th√¥ng minh
    """
    logging.info(f"üîç Semantic search: {query}")

    try:
        query_vec = model.encode(query).tolist()
        query_clean = normalize_text(query)
        query_expanded = normalize_and_expand(query_clean)

        # üîß TRUY V·∫§N TH√äM filename ƒë·ªÉ x·ª≠ l√Ω ƒë√∫ng
        cursor.execute("SELECT title, content, embedding, title FROM forms")
        rows = cursor.fetchall()

        def cosine_similarity(a, b):
            a = np.array(a, dtype=np.float32)
            b = np.array(eval(b), dtype=np.float32) if isinstance(b, str) else np.array(b, dtype=np.float32)
            norm = (np.linalg.norm(a) * np.linalg.norm(b))
            return float(np.dot(a, b) / norm) if norm != 0 else 0.0

        results = []
        COSINE_THRESHOLD = 0.6

        for title, content, embedding, filename in rows:
            title_clean = normalize_text(title)
            content_clean = normalize_text(content)
            file_clean = normalize_text(filename or "")
            full_text = f"{title_clean} {content_clean}"

            score = 0.0
            reason = ""

            # ‚úÖ ∆Øu ti√™n n·∫øu truy v·∫•n g·∫ßn gi·ªëng t√™n file th·ª±c s·ª± (ch·ª©a ".doc" ho·∫∑c r√µ ƒë·ªãnh d·∫°ng)
            if "." in query_clean or query_clean.endswith("docx") or query_clean == file_clean:
                if query_clean in file_clean:
                    score = 1.0
                    reason = "Kh·ªõp g·∫ßn ƒë√∫ng t√™n file"
            # ‚úÖ ∆Øu ti√™n ti√™u ƒë·ªÅ
            elif query_expanded in title_clean:
                score = 0.98
                reason = "Kh·ªõp g·∫ßn ƒë√∫ng ti√™u ƒë·ªÅ"
            # ‚úÖ ∆Øu ti√™n n·ªôi dung (s√¢u)
            elif query_expanded in content_clean:
                score = 0.9
                reason = "Kh·ªõp to√†n vƒÉn kh√¥ng d·∫•u"
            elif any(word in content_clean for word in query_expanded.split()):
                score = 0.8
                reason = "M·ªôt s·ªë t·ª´ tr√πng trong n·ªôi dung"
            else:
                score = cosine_similarity(query_vec, embedding)
                reason = "Hi·ªÉu √Ω ƒë·∫°i di·ªán (SBERT)"

            if score >= COSINE_THRESHOLD:
                # üîç T·∫°o ƒëo·∫°n tr√≠ch li√™n quan t·ª´ n·ªôi dung
                sentences = re.split(r'[.?!]\s+', content)
                matched = [s for s in sentences if query_expanded in normalize_text(s)]
                snippet = " ".join(matched)[:300] + "..." if matched else content[:300] + "..."

                results.append({
                    "title": title,
                    "snippet": snippet,
                    "similarity_score": round(score, 4),
                    "reason": reason
                })

        sorted_results = sorted(results, key=lambda x: x["similarity_score"], reverse=True)[:top_k]

        return {
            "query": query,
            "top_matches": sorted_results
        }

    except Exception as e:
        logging.error(f"‚ùå Search error: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": "Search failed", "detail": str(e)}
        )


@app.post("/top-k")
def top_k_search(query: str = Body(...), k: int = Body(...)):
    logging.info(f"üîç ƒêang t√¨m ki·∫øm v·ªõi truy v·∫•n: {query}, Top {k} k·∫øt qu·∫£")
    try:
        query_embedding = model.encode(query).tolist()
        cursor.execute("""
            SELECT title, content
            FROM forms
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_embedding, k))
        results = cursor.fetchall()
        if results:
            return {
                "query": query,
                "top_k_results": [
                    {"title": result[0], "content": result[1][:500] + "..."} for result in results
                ]
            }
        else:
            return {
                "query": query,
                "message": f"Kh√¥ng t√¨m th·∫•y {k} bi·ªÉu m·∫´u ph√π h·ª£p."
            }
    except Exception as e:
        logging.error(f"L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm: {e}")
        cursor.execute("ROLLBACK;")
        return {"error": f"Top-K search failed: {e}"}

@app.post("/get-embedding")
def get_embedding(text: Union[str, List[str]] = Body(...)):
    try:
        if isinstance(text, str):
            text = [text]
        text = [t[:5000] for t in text]
        embeddings = model.encode(text).tolist()
        return {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings
        }
    except Exception as e:
        logging.error(f"L·ªói khi sinh embedding: {e}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"L·ªói khi sinh embedding: {e}"}
        )
